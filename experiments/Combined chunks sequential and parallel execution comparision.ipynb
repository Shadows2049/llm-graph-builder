{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb375284",
   "metadata": {},
   "source": [
    "# Creating Knowledge graphs from Pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e44b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"\"\n",
    "username =\"\"\n",
    "password = \"\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98e7e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Add OpenAI key\n",
    "#os.environ['OPENAI_API_KEY']=''\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "# class Source(BaseSource):\n",
    "#     properties: Optional[List[Property]] = Field(\n",
    "#         None, description=\"List of sources\"\n",
    "#     )        \n",
    "        \n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )\n",
    "    \n",
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6d19f",
   "metadata": {},
   "source": [
    "# Using GPT-3.5-turbo-16k model to create the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "505a5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    input,\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-3.5\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cf8b15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(document.page_content,nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])\n",
    "    return graph_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b68b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata \n",
    "\n",
    "def filter_text(text):\n",
    "    # regex pattern to match ascii characters and currency symbols\n",
    "    pattern = re.compile(r'[^\\x20-\\x7E\\u20A0-\\u20CF\\u20E0-\\u20FF]')\n",
    "    # Use the pattern to filter out non-English characters\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7552cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"../data/new_york_city_example_itinerary.pdf\")\n",
    "\n",
    "pages = loader.load()\n",
    "\n",
    "for page in pages:\n",
    "    page.page_content = filter_text(page.page_content)\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# Only take the first 4 pages of the document\n",
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e98946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0e0d1",
   "metadata": {},
   "source": [
    "Replacing currency symbols with respective unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c12101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "        currency_symbol = None  \n",
    "        for char in doc.page_content:  \n",
    "                if unicodedata.category(char) == \"Sc\":  \n",
    "                        currency_symbol = char  \n",
    "                        break  \n",
    "        # Convert the currency symbol to a Unicode escape sequence  \n",
    "        if currency_symbol is not None:  \n",
    "                encoded_symbol = \"\\\\u\" + hex(ord(currency_symbol))[2:].zfill(4)  \n",
    "                doc.page_content = doc.page_content.replace(currency_symbol, encoded_symbol)  \n",
    "                print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40640370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_and_relations_count(graph_document_list: List[GraphDocument]):\n",
    "    distinct_nodes = set()\n",
    "    relations = []\n",
    "\n",
    "    for graph_document in graph_document_list:\n",
    "        #get distinct nodes\n",
    "        for node in graph_document.nodes:\n",
    "                node_id = node.id\n",
    "                node_type= node.type\n",
    "                if (node_id, node_type) not in distinct_nodes:\n",
    "                    distinct_nodes.add((node_id, node_type))\n",
    "        #get all relations\n",
    "        for relation in graph_document.relationships:\n",
    "                relations.append(relation.type)\n",
    "            \n",
    "    print(\"nodes = \",len(distinct_nodes))   \n",
    "    print(\"relations = \",len(relations)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e8f2c",
   "metadata": {},
   "source": [
    "Sequencial Processing of chunks (chunk size 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14a6fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:20<00:00, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:03:20.005178\n",
      "nodes =  108\n",
      "relations =  107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "distinct_nodes = set()\n",
    "relations = []\n",
    "graph_document_list=[]\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    graph_document=extract_and_store_graph(d)\n",
    "\n",
    "    graph_document_list.append(graph_document)\n",
    "\n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9bd6c",
   "metadata": {},
   "source": [
    "Parallel Processing chunk with ThreadPool size 8 (chunk size 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3bd6ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 168.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:00:28.000194\n",
      "nodes =  108\n",
      "relations =  101\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "start_time = datetime.now()\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    \n",
    "    for i,chunk in tqdm(enumerate(documents), total=len(documents)):\n",
    "        futures.append(executor.submit(extract_and_store_graph,chunk))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        graph_document_list.append(future.result())\n",
    "    \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4adf01",
   "metadata": {},
   "source": [
    "Combined 2 chunks and sequential processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3494bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:41<00:00, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:01:41.191927\n",
      "nodes =  76\n",
      "relations =  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "combined_chunk=''\n",
    "combined_chunk_document_list=[]\n",
    "   \n",
    "combined_chunks = [\"\".join(document.page_content for document in documents[i:i+2]) for i in range(0, len(documents),2)]\n",
    "\n",
    "for i in range(len(combined_chunks)):\n",
    "        combined_chunk_document_list.append(Document(page_content=combined_chunks[i]))       \n",
    "        \n",
    "start_time = datetime.now()\n",
    "     \n",
    "for i, d in tqdm(enumerate(combined_chunk_document_list), total=len(combined_chunk_document_list)):\n",
    "    graph_document=extract_and_store_graph(d)\n",
    "    graph_document_list.append(graph_document)\n",
    "        \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e457aef",
   "metadata": {},
   "source": [
    "Combined 2 chunks and Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f14f9b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 72.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:00:41.756752\n",
      "nodes =  77\n",
      "relations =  54\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "combined_chunk=''\n",
    "combined_chunk_document_list=[]\n",
    "   \n",
    "combined_chunks = [\"\".join(document.page_content for document in documents[i:i+2]) for i in range(0, len(documents),2)]\n",
    "\n",
    "start_time = datetime.now()\n",
    "     \n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for i in range(len(combined_chunks)):\n",
    "        combined_chunk_document_list.append(Document(page_content=combined_chunks[i]))\n",
    " \n",
    "    for i,chunk in tqdm(enumerate(combined_chunk_document_list), total=len(combined_chunk_document_list)):\n",
    "        futures.append(executor.submit(extract_and_store_graph,chunk))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        graph_document_list.append(future.result())\n",
    "        \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97fe1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../data/new_york_city_example_itinerary.pdf\")\n",
    "pages = loader.load()\n",
    "for page in pages:\n",
    "    page.page_content = filter_text(page.page_content)\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa1f5b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d711bf8",
   "metadata": {},
   "source": [
    "Sequential processing of chunk (Chunk size 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "89901d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:53<00:00, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:01:53.235920\n",
      "nodes =  71\n",
      "relations =  69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "distinct_nodes = set()\n",
    "relations = []\n",
    "graph_document_list=[]\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    graph_document=extract_and_store_graph(d)\n",
    "\n",
    "    graph_document_list.append(graph_document)\n",
    "\n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9378cbb",
   "metadata": {},
   "source": [
    "Parallel Processing chunk with ThreadPool size 8 (chunk size 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3f8614e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 132.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:00:25.987004\n",
      "nodes =  70\n",
      "relations =  59\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "start_time = datetime.now()\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    \n",
    "    for i,chunk in tqdm(enumerate(documents), total=len(documents)):\n",
    "        futures.append(executor.submit(extract_and_store_graph,chunk))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        graph_document_list.append(future.result())\n",
    "    \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c89eec",
   "metadata": {},
   "source": [
    "Combined 2 chunks and sequential processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f25a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:46<00:00, 26.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:01:46.847089\n",
      "nodes =  53\n",
      "relations =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "combined_chunk=''\n",
    "combined_chunk_document_list=[]\n",
    "   \n",
    "combined_chunks = [\"\".join(document.page_content for document in documents[i:i+2]) for i in range(0, len(documents),2)]\n",
    "\n",
    "for i in range(len(combined_chunks)):\n",
    "        combined_chunk_document_list.append(Document(page_content=combined_chunks[i]))       \n",
    "        \n",
    "start_time = datetime.now()\n",
    "     \n",
    "for i, d in tqdm(enumerate(combined_chunk_document_list), total=len(combined_chunk_document_list)):\n",
    "    graph_document=extract_and_store_graph(d)\n",
    "    graph_document_list.append(graph_document)\n",
    "        \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c323e",
   "metadata": {},
   "source": [
    "Combined 2 chunks and Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ce092689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 135.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing_time =  0:00:22.058208\n",
      "nodes =  53\n",
      "relations =  18\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "graph_document_list=[]\n",
    "futures=[]\n",
    "combined_chunk=''\n",
    "combined_chunk_document_list=[]\n",
    "   \n",
    "combined_chunks = [\"\".join(document.page_content for document in documents[i:i+2]) for i in range(0, len(documents),2)]\n",
    "\n",
    "start_time = datetime.now()\n",
    "     \n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for i in range(len(combined_chunks)):\n",
    "        combined_chunk_document_list.append(Document(page_content=combined_chunks[i]))\n",
    " \n",
    "    for i,chunk in tqdm(enumerate(combined_chunk_document_list), total=len(combined_chunk_document_list)):\n",
    "        futures.append(executor.submit(extract_and_store_graph,chunk))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        graph_document_list.append(future.result())\n",
    "        \n",
    "end_time = datetime.now()  \n",
    "print(\"Processing_time = \", end_time-start_time) \n",
    "get_nodes_and_relations_count(graph_document_list)         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
